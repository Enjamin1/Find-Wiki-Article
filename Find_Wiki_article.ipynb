{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Find Wiki article",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPeolUpLztzF4ad27v1HJvi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Enjamin1/Find-Wiki-Article/blob/main/Find_Wiki_article.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9fIYKRFJaIo"
      },
      "source": [
        "def search_web(x):\r\n",
        "  import urllib.request, urllib.parse, urllib.error\r\n",
        "  from bs4 import BeautifulSoup\r\n",
        "  url = x\r\n",
        "\r\n",
        "  user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\r\n",
        "\r\n",
        "  #url = \"https://www.yesbank.in/pdf/forexcardratesenglish_pdf\"\r\n",
        "  headers={'User-Agent':user_agent,}\r\n",
        "  request=urllib.request.Request(url,None,headers)\r\n",
        "  r = urllib.request.urlopen(request).read().decode()\r\n",
        "  soup = BeautifulSoup(r, 'html.parser')\r\n",
        "  return soup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIORSG80lcom"
      },
      "source": [
        "def search_wikipeida():\r\n",
        "  import sys\r\n",
        "  import random\r\n",
        "  import re\r\n",
        "\r\n",
        "  count = 0\r\n",
        "\r\n",
        "  wiki_article = input(\"Please enter wikipeida article url\")\r\n",
        "  if wiki_article.startswith(\"https://en.wikipedia.org/\"):\r\n",
        "    print('Lets take a look at that article')\r\n",
        "  else:\r\n",
        "    print(\"'Plsease enter a url that begins with 'https://en.wikipedia.org/'\")\r\n",
        "    \r\n",
        "  word = input(\"Please Enter the word you would like to Search for:\")\r\n",
        "\r\n",
        "\r\n",
        "  while True:\r\n",
        "\r\n",
        "\r\n",
        "    start_page = (search_web(wiki_article))\r\n",
        "\r\n",
        "    \r\n",
        "    \r\n",
        "    print(\"Heading to\", wiki_article, \"please give me a moment.\")\r\n",
        "    print(\"This was\", count, \"pages away from the one I started at.\")\r\n",
        "    count+=1\r\n",
        "\r\n",
        "\r\n",
        "    list_ = list()\r\n",
        "    list1 = list()\r\n",
        "    ryans_list = list()\r\n",
        "    flist = list()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    #extract the a tags which hold the href tags then seperate the tags and append them to a list for further use later\r\n",
        "    atags = start_page.find_all('a')\r\n",
        "\r\n",
        "    for tag in atags:\r\n",
        "      ataghref = tag.get('href')\r\n",
        "      try:\r\n",
        "        if not ataghref.startswith ('/wiki/'):\r\n",
        "          continue\r\n",
        "      except:\r\n",
        "        continue\r\n",
        "      list_.append(ataghref)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    #turn the url into a string to later be joined with the href found\r\n",
        "    base = str('https://en.wikipedia.org')\r\n",
        "\r\n",
        "    for special_tag in list_:\r\n",
        "\r\n",
        "    #create a list with the stringed and href values in it then join them together\r\n",
        "      list1 = [base, special_tag]\r\n",
        "      new_url = (\"\".join(list1))\r\n",
        "      flist.append(new_url)\r\n",
        "\r\n",
        "    #Removed the last three links which are administrative from the page\r\n",
        "    flist = flist[:-4]\r\n",
        "\r\n",
        "    # list for ryan to search with neural net\r\n",
        "    for item in flist:\r\n",
        "        page = (item[30:].replace('_',' '))\r\n",
        "        cleaned_words = re.sub('[^A-Za-z0-9 ]+', '', page)\r\n",
        "        ryans_list.append(cleaned_words)\r\n",
        "\r\n",
        "   \r\n",
        "    # Create a dictionary of the links in the article and sorts them by which appears most in the list.\r\n",
        "    d = dict()\r\n",
        "    for url in flist:\r\n",
        "      d[url] = d.get(url, 0) + 1\r\n",
        "    sort_d = sorted(d.items(), key=lambda x: x[1], reverse=True)\r\n",
        "    most_common_link = sort_d[0]\r\n",
        "       \r\n",
        "\r\n",
        "    print(\"The most common link in that article was\", most_common_link[0], \"and it appeared\", most_common_link[1], \"times\")\r\n",
        "\r\n",
        "          \r\n",
        "\r\n",
        "    \r\n",
        "   \r\n",
        "    \r\n",
        "\r\n",
        "    #Search list_ of referenced pages in article to see if the one you want is in the page\r\n",
        "    for item in flist:\r\n",
        "      page = (item[30:].replace('_',' '))\r\n",
        "\r\n",
        "      if page == word:\r\n",
        "        print(\"We Found the page for\",word, \"among the links in the page. It was link number\", count, \"on the page. See Below for link.\", '\\n', item)\r\n",
        "        print('\\n')\r\n",
        "        exit()\r\n",
        "\r\n",
        "      else:\r\n",
        "        new_page = (random.choice(flist))\r\n",
        "    wiki_article = new_page\r\n",
        "    print(\"This article had \", len(ryans_list), \"links in it but none of them were\", word,\".\")\r\n",
        "    print('\\n')\r\n",
        "    continue\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFLAu753L6Gs"
      },
      "source": [
        "search_wikipeida()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}